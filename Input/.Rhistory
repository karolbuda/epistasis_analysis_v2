geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] + 2*error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - 2*error_terms[2], lty = 2) +
theme_classic()
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
error_terms = error_rates %>%
filter(pheno_sd != 0) %>%
ungroup() %>%
summarise(mean_sd = mean(pheno_sd),
sd_sd = sd(pheno_sd)) %>%
unlist(., use.names = F)
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] + 2*error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - 2*error_terms[2], lty = 2) +
theme_classic()
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
error_terms = error_rates %>%
filter(pheno_sd != 0) %>%
ungroup() %>%
summarise(mean_sd = mean(pheno_sd),
sd_sd = sd(pheno_sd)) %>%
unlist(., use.names = F)
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
theme_classic()
error_rates %>%
filter(pheno_sd != 0)
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
theme_classic()
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_histogram() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
theme_classic()
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
theme_classic()
error_rates %>%
filter(pheno_sd != 0) %>%
)
View(error_rates %>% filter(pheno_sd != 0))
error_terms[1] - error_terms[2]
rnorm(10, 10, 0.1)
rnorm(10, 100, 0.1)
rnorm(10, 100, log10(1.5)*2)
sd(rnorm(10, 100, log10(1.5)*2))
sd(rnorm(10, 100, log10(1.5)*2))/2
sd(rnorm(10, 100, log10(1.5)*2))/2
sd(rnorm(10, 100, log10(1.5)*2))/2
sd(rnorm(10, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
sd(rnorm(3, 100, log10(1.5)*2))/2
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
error_terms = error_rates %>%
filter(pheno_sd != 0) %>%
ungroup() %>%
summarise(mean_sd = mean(pheno_sd),
sd_sd = sd(pheno_sd)) %>%
unlist(., use.names = F)
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = error_terms[1] - error_terms[2], lty = 2) +
theme_classic()
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
error_terms = error_rates %>%
filter(pheno_sd != 0) %>%
ungroup() %>%
summarise(mean_sd = mean(pheno_sd),
sd_sd = sd(pheno_sd)) %>%
unlist(., use.names = F)
error_rates %>%
filter(pheno_sd != 0) %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
geom_vline(xintercept = error_terms[1]) +
geom_vline(xintercept = error_terms[1] + error_terms[2], lty = 2) +
geom_vline(xintercept = 0, lty = 2) +
theme_classic()
sum(error_rates$pheno_sd > log10(1.5)) / length(error_rates$pheno_sd)
error_rates
sum(error_rates[,3 != 0]$pheno_sd > log10(1.5)) / length(error_rates[,3 != 0]$pheno_sd)
sum(error_rates$pheno_sd > log10(1.5)) / length(error_rates$pheno_sd)
sum(error_rates[,3 != 0]$pheno_sd > log10(1.5)) / length(error_rates[,3 != 0]$pheno_sd)
error_rates[,3 != 0]
error_rates[,3] != 0
error_rates[error_rates[,3] != 0, []
error_rates[error_rates[,3] != 0,
]
sum(error_rates$pheno_sd > log10(1.5)) / length(error_rates$pheno_sd)
sum(error_rates[error_rates[,3] != 0, ]$pheno_sd > log10(1.5)) / length(error_rates[error_rates[,3] != 0, ]$pheno_sd)
sum(error_rates$pheno_sd > log10(2)) / length(error_rates$pheno_sd)
sum(error_rates[error_rates[,3] != 0, ]$pheno_sd > log10(2)) / length(error_rates[error_rates[,3] != 0, ]$pheno_sd)
pheno_sd
error_rates$pheno_sd
mode(error_rates$pheno_sd)
median(error_rates$pheno_sd)
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
median(error_rates$pheno_sd)
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
mean(error_rates$pheno_sd)
error_rates %>%
ggplot(aes(x = pheno_sd)) %>%
geom_density() +
theme_classic()
error_rates %>%
ggplot(aes(x = pheno_sd)) +
geom_density() +
theme_classic()
error_rates %>%
ggplot(aes(x = pheno_sd)) +
geom_histogram() +
theme_classic()
error_rates %>%
ggplot(aes(x = pheno_sd)) +
geom_histogram() +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic()
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
error_rates %>%
ggplot(aes(x = pheno_sd)) +
geom_histogram() +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
error_rates$pheno_sd > log10(1.5)
error_rates$pheno_sd*2
sum(error_rates$pheno_sd*2 <= log10(1.5)) / length(error_rates$pheno_sd)
error_rates %>%
ggplot(aes(x = pheno_sd*2)) +
geom_histogram() +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
error_rates %>%
ggplot(aes(x = pheno_sd*2)) +
geom_histogram(bins = 50) +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
sum(error_rates$pheno_sd*2 <= log10(1.5)) / length(error_rates$pheno_sd)
median(error_rates$pheno_sd*2)
10^0.1705538
error_rates
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),]
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),]
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(sample = pheno_mean)) +
stat_qq() +
stat_qq_line() +
facet_wrap(~ unique_id)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),]
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(sample = Phenotype)) +
stat_qq() +
stat_qq_line() +
facet_wrap(~ unique_id)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(x = Phenotype)) +
geom_histogram() +
facet_wrap(~ unique_id)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(x = Phenotype)) +
geom_histogram() +
facet_wrap(~ unique_id)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(sample = Phenotype)) +
stat_qq() +
stat_qq_line() +
facet_wrap(~ unique_id)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),]
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F)
error_set[which(duplicated(error_set %>%
error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F)
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),]
error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
ggplot(aes(x = Phenotype)) +
geom_histogram() +
facet_wrap(~ Genotype)
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz"))) #%>% select(-c('2', '3'))
## Make fit_land have IDs by finding where muts are 0, i.e. start of new dataset, and multiplying
## Those values by the folder names to generate ID vector
fit_land_spots = c(which(fit_land$muts == 0), (dim(fit_land)[1] + 1)) # Add end of dataset
fit_land_counts = c()
for(i in 1:(length(fit_land_spots) - 1)) {
if(i == 1) {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - 1)
} else {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - fit_land_spots[i])
}
}
## This assumes folders are read in the order that they are listed
fit_land_ids = rep(str_remove_all(list.files(pattern="observed_values.csv", recursive = T), "/observed_values.csv"), fit_land_counts)
fit_land$unique_id = fit_land_ids
fit_land
# Briefly change to Input folder for assessing error rates
knitr::opts_knit$set(root.dir = "/Users/karolbuda/OneDrive - UBC/PhD/Epistasis_Lit/Bulk/Input")
library(tidyverse)
library(ggExtra)
library(ggrepel)
library(ggpubr)
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
## % of SDs for measurements that are less than 1.5-fold threshold
sum(error_rates$pheno_sd*2 <= log10(1.5)) / length(error_rates$pheno_sd)
median(error_rates$pheno_sd*2)
error_rates %>%
ggplot(aes(x = pheno_sd*2)) +
geom_histogram(bins = 50) +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
knitr::opts_knit$set(root.dir = "/Users/karolbuda/OneDrive - UBC/PhD/Epistasis_Lit/Bulk/Output")
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz"))) #%>% select(-c('2', '3'))
## Make fit_land have IDs by finding where muts are 0, i.e. start of new dataset, and multiplying
## Those values by the folder names to generate ID vector
fit_land_spots = c(which(fit_land$muts == 0), (dim(fit_land)[1] + 1)) # Add end of dataset
fit_land_counts = c()
for(i in 1:(length(fit_land_spots) - 1)) {
if(i == 1) {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - 1)
} else {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - fit_land_spots[i])
}
}
## This assumes folders are read in the order that they are listed
fit_land_ids = rep(str_remove_all(list.files(pattern="observed_values.csv", recursive = T), "/observed_values.csv"), fit_land_counts)
fit_land$unique_id = fit_land_ids
fit_land
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz"))) %>% select(-c('2', '3'))
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz"))) %>% select(-c(4, 5))
do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz")))
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz")))
fit_land
fit_land %>% select(-c('2', '3'))
fit_land %>% select(c('2', '3'))
fit_land %>% select(2:3)
?select
fit_land %>% select(id)
fit_land %>% dplyr::select(id)
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "2", "3", "enz"))) %>% dplyr::select(-c('2', '3'))
fit_land
fit_land = do.call("rbind", lapply(list.files(pattern="observed_values.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "junk1", "junk2", "enz"))) %>% dplyr::select(-c('junk1', 'junk2'))
## Make fit_land have IDs by finding where muts are 0, i.e. start of new dataset, and multiplying
## Those values by the folder names to generate ID vector
fit_land_spots = c(which(fit_land$muts == 0), (dim(fit_land)[1] + 1)) # Add end of dataset
fit_land_counts = c()
for(i in 1:(length(fit_land_spots) - 1)) {
if(i == 1) {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - 1)
} else {
fit_land_counts = c(fit_land_counts, fit_land_spots[i + 1] - fit_land_spots[i])
}
}
## This assumes folders are read in the order that they are listed
fit_land_ids = rep(str_remove_all(list.files(pattern="observed_values.csv", recursive = T), "/observed_values.csv"), fit_land_counts)
fit_land$unique_id = fit_land_ids
fit_land
fit_land %>%
ggplot(aes(x = effect)) +
geom_histogram(bins = 20, alpha = 0.8) +
facet_wrap(~ enz, scales = "free") +
theme_classic() #+
#theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
#      axis.title.y = element_blank(),
#      axis.title.x = element_blank())
all_ratios = do.call("rbind", lapply(list.files(pattern="ratio_export.csv", recursive = T), read_csv, skip = 1, show_col_types=F, col_names = c("id", "effect", "muts", "cv", "colors")))
all_ratios = all_ratios %>% filter(muts > 1)
length(all_ratios$effect) # examined muts
## Reads all csvs in collated folder and combines them
d = do.call("rbind", lapply(list.files(pattern="*2d_box.csv", recursive = T), read_csv, show_col_types=F, col_names = c("positions", "identity", "mut", "effects", "enzyme", "type", "cond")))
d1 = d %>% filter_all(any_vars(!is.na(.))) # removes rows with all NA before making unique ID
d1 = d1 %>%
unite("unique_id", c(positions, enzyme, type, cond), remove = F)
d1
length(unique(d1$positions))
length(unique(d1$unique_id))
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
# Briefly change to Input folder for assessing error rates
knitr::opts_knit$set(root.dir = "/Users/karolbuda/OneDrive - UBC/PhD/Epistasis_Lit/Bulk/Input")
library(tidyverse)
library(ggExtra)
library(ggrepel)
library(ggpubr)
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
## % of SDs for measurements that are less than 1.5-fold threshold
sum(error_rates$pheno_sd*2 <= log10(1.5)) / length(error_rates$pheno_sd)
median(error_rates$pheno_sd*2)
max(error_rates$pheno_sd*2)
error_rates %>%
ggplot(aes(x = pheno_sd*2)) +
geom_histogram(bins = 50) +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
file_vector = str_remove(list.files(pattern="*.csv", recursive = T), ".csv")
error_set = lapply(list.files(pattern="*.csv", recursive = T), read_csv, skip = 5, col_names = F, col_select = 1:2, show_col_types=F)
for(i in 1:length(error_set)){
error_set[[i]][,3] = file_vector[i]
}
error_set = do.call("rbind", error_set)
colnames(error_set) = c("Genotype", "Phenotype", "unique_id")
error_rates = error_set[which(duplicated(error_set %>%
unite(unique_geno, c(1,3), sep = "_", remove = F) %>%
pull(unique_geno))),] %>%
group_by(Genotype) %>%
summarise(pheno_mean = mean(Phenotype),
pheno_sd = sd(Phenotype))
## % of SDs for measurements that are less than 1.5-fold threshold
sum(error_rates$pheno_sd*2 <= log10(1.5)) / length(error_rates$pheno_sd)
median(error_rates$pheno_sd*2)
summary(error_rates$pheno_sd*2)
error_rates %>%
ggplot(aes(x = pheno_sd*2)) +
geom_histogram(bins = 50) +
geom_vline(xintercept = log10(1.5), lty = 2) +
theme_classic() +
theme(text = element_text(size=18), axis.text = element_text(size = 16, color = "black"),
axis.title.y = element_blank(),
axis.title.x = element_blank(),
legend.position = "none")
10^0.74638
plot(0:10 * 10:0 * 0:10)
error_rates$pheno_sd
error_rates$pheno_sd*2
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(error_rates$pheno_sd*2)
getmode((error_rates %>% filter(pheno_sd != 0) %>% pull(pheno_sd)) * 2)
10^getmode((error_rates %>% filter(pheno_sd != 0) %>% pull(pheno_sd)) * 2)
